---
name: research-synthesis
description: Conduct user research, synthesize findings, and deliver actionable insights. Use for discovery interviews, validation testing, competitive analysis, research planning, and research reports.
---

# Research & Synthesis

Research & Synthesis is the discipline of understanding users deeply through structured inquiry and turning raw observations into actionable insights. It spans discovery, validation, and synthesis — from uncovering user problems to testing whether solutions resonate.

---

## Core Principles

1. **Research before recommendations.** Never skip straight to solutions. Understand the problem space first — the best recommendations come from deep understanding.

2. **Evidence over opinion.** Ground every insight in data. Quote users. Show your work. If you can't point to evidence, it's an assumption.

3. **Actionable or it's not an insight.** Every finding must answer "so what?" If it doesn't lead to a decision or action, dig deeper or reframe.

4. **Honest findings, even when uncomfortable.** Report what you found, not what stakeholders want to hear. Clients hire you for truth, not validation.

5. **Appropriate rigor for the stakes.** Match methodology to the question. Don't over-engineer small decisions or under-research big bets.

6. **Past behavior, not hypotheticals.** People can't predict what they'll do. Ask what they actually did. "Tell me about the last time..." beats "Would you ever..."

7. **Follow the energy.** When participants lean in or get animated, go deeper. That's where the real insights live.

### Industry Principles

8. **The right method for the right question.** Discovery for understanding problems. Validation for testing solutions. Don't conflate them.

9. **Sample size matters, but not infinitely.** You'll hear the main themes after 5 interviews. More adds nuance but not new themes.

10. **Synthesis is where value is created.** Raw notes aren't insights. The work of organizing, patterning, and interpreting is where research earns its value.

11. **Confidence levels matter.** "5 of 5 users struggled" is different from "2 of 5 mentioned." State your confidence.

12. **Research is a team sport.** Include stakeholders in synthesis. Insights they help discover stick better than insights you present.

---

## Workflow

> Follow the [Workflow Orchestration Pattern](../../_FRAMEWORKS/workflow-orchestration.md) for the universal workflow approach. Below are the research-specific details for each step.

### 1. Clarify Before Starting

Before conducting any research, answer:

- **What decision are we trying to make?** Specific question, not vague curiosity
- **What do we need to learn?** What information would change the decision?
- **Who are we researching?** Specific user segment or persona
- **What do we already know?** Existing data, past research, assumptions
- **What's the timeline?** When does this decision need to be made?
- **What's the output?** Report, presentation, working session?

### 2. Load Relevant Context

**Core context (always load):**
- `core/identity.md` — Product, positioning, audiences, terminology
- `core/voice.md` — Tone, language rules for written deliverables

**Load from templates/:**
- [Interview Guide](templates/interview-guide.md) — When creating interview guides
- [Research Report](templates/research-report.md) — When writing research reports
- [Validation Report](templates/validation-report.md) — When writing validation reports

### 3. Choose the Right Method

| Goal | Method | Sample Size |
|------|--------|-------------|
| Understand problems and needs | Discovery interviews | 6-12 |
| Test if a solution resonates | Validation interviews | 5-8 |
| Test usability of existing product | Usability testing | 5-7 |
| Quantify attitudes or preferences | Survey | 100+ |
| Understand competitive landscape | Competitive analysis | 5-10 competitors |
| Explore market opportunity | Market research | Varies |

### 4. Prepare Research Materials

**For interviews:**
1. Define participant criteria (who qualifies)
2. Create screener questions (how to find them)
3. Write interview guide (questions and flow)
4. Plan logistics (scheduling, recording, notes)

**For validation:**
1. Clarify what you're testing (concept, prototype, messaging)
2. Confirm the problem exists before testing solution
3. Prepare materials (mockups, descriptions, stimuli)
4. Define success criteria (what would "work" look like?)

### 5. Conduct Research

**During sessions:**
- Stay neutral — don't lead or validate
- Ask about past behavior, not hypotheticals
- Follow the energy — go deeper when they engage
- Capture verbatim quotes (exact words matter)
- Note non-verbal reactions and hesitations
- Let silence work — don't fill every pause

**Questions to always ask:**
- "Can you give me a specific example?"
- "What happened next?"
- "How did that make you feel?"
- "What would have made that easier?"

### 6. Synthesize Findings

**The synthesis ladder:**

| Level | Question | Example |
|-------|----------|---------|
| Observation | What did you see/hear? | "3 of 5 users clicked Settings first" |
| Pattern | What keeps coming up? | "Users expect settings to be primary navigation" |
| Insight | What does this mean? | "Users come with specific tasks, not to explore" |
| Implication | So what should we do? | "Optimize for task completion, not discovery" |

**Synthesis process:**
1. Review all notes and recordings
2. Extract observations onto cards/stickies
3. Group by theme (affinity mapping)
4. Name each theme
5. Climb the ladder for each theme
6. Identify implications and recommendations

### 7. Deliver Findings

**Match deliverable to need:**

| Need | Deliverable | Length |
|------|-------------|--------|
| Inform decision-makers | Research report | 5-10 pages |
| Quick stakeholder alignment | Executive summary | 1 page |
| Validate direction | Validation report | 3-5 pages |
| Prioritize roadmap | Prioritization framework | 2-3 pages |
| Enable ongoing use | Insight repository | Living document |

**All deliverables should include:**
- Research question and methodology
- Key findings with evidence
- Confidence levels
- Specific, actionable recommendations
- Next steps

---

## Competitive Intelligence

Competitive analysis uses research methodology to understand market context. It informs positioning, feature prioritization, and strategic decisions.

### When to Use Competitive Analysis

- Entering a new market or category
- Positioning or repositioning a product
- Prioritizing features (what do competitors have/lack?)
- Understanding market expectations
- Identifying differentiation opportunities

### Competitive Analysis Process

**1. Define the scope:**
- Direct competitors (same problem, same solution)
- Indirect competitors (same problem, different solution)
- Adjacent players (different problem, could expand)

**2. Gather information:**

| Source | What You Learn |
|--------|----------------|
| Product (sign up, use it) | UX, features, onboarding |
| Marketing (website, ads) | Positioning, messaging, ICP |
| Reviews (G2, Capterra, App Store) | Strengths, weaknesses, user complaints |
| Social/community | Brand perception, user sentiment |
| Job postings | Strategic priorities, tech stack |
| Pricing pages | Business model, packaging |
| Press/funding | Stage, resources, direction |

**3. Analyze systematically:**

| Dimension | Questions |
|-----------|-----------|
| **Positioning** | Who are they for? What problem do they solve? How do they describe themselves? |
| **Features** | What capabilities do they offer? What's missing? What's differentiated? |
| **Experience** | How easy to use? What's the onboarding like? Where does it break down? |
| **Pricing** | How do they charge? What's included at each tier? Where's the value? |
| **Strengths** | What do users praise? Where do they win? |
| **Weaknesses** | What do users complain about? Where do they lose? |

**4. Synthesize into insights:**
- Where is there market consensus? (table stakes features)
- Where is there differentiation opportunity?
- What are competitors missing that users need?
- How should we position relative to alternatives?

### Competitive Analysis Output

**Feature comparison matrix:**
```
| Feature | Us | Competitor A | Competitor B |
|---------|-----|--------------|--------------|
| [Feature] | Yes | Yes | No |
| [Feature] | Partial | Yes | Yes |
```

**Positioning map:**
- 2x2 with meaningful axes (e.g., simple↔powerful, SMB↔enterprise)
- Show where competitors cluster and where whitespace exists

**Competitive brief:**
- 1-2 pages per major competitor
- Positioning, strengths, weaknesses, opportunity for us
- Evidence-backed (quotes from reviews, specific observations)

### Competitive Intelligence Principles

- **Use the product.** Reading about it isn't enough. Sign up, try the flows.
- **Talk to their users.** Reviews are good. Interviews are better.
- **Focus on insight, not just data.** A feature list isn't useful. What it means for us is.
- **Update regularly.** Competitors change. One-time analysis goes stale.
- **Don't obsess.** Competitive awareness is healthy. Competitive fixation is not.

---

## Quality Checks

### Research Quality

- [ ] Research question is clearly stated?
- [ ] Method matches the question being asked?
- [ ] Sample size is sufficient (minimum 5 for qualitative)?
- [ ] Participants match target criteria?
- [ ] Questions are open and non-leading?
- [ ] Both confirming and disconfirming evidence sought?

### Synthesis Quality

- [ ] Findings are grounded in evidence (quotes, data)?
- [ ] Patterns based on multiple data points (not one person)?
- [ ] Insights answer "so what?"?
- [ ] Counter-evidence acknowledged?
- [ ] Confidence levels stated?
- [ ] Assumptions separated from findings?

### Deliverable Quality

- [ ] Executive summary captures key points?
- [ ] Evidence supports all conclusions?
- [ ] Recommendations are specific and actionable?
- [ ] Format matches audience needs?
- [ ] Next steps are clear?
- [ ] Methodology is documented?

---

## Anti-Patterns

| Anti-Pattern | Problem | Instead |
|--------------|---------|---------|
| **Leading questions** | Get answers you want, not truth | Ask open, neutral questions |
| **Confirmation bias** | Only see supporting evidence | Actively seek disconfirming data |
| **Solution-first** | Test solution before validating problem | Always confirm problem first |
| **Hypothetical questions** | People can't predict behavior | Ask about past behavior |
| **Cherry-picking quotes** | Misleading conclusions | Show the full picture |
| **Vague recommendations** | No clear action | Every insight needs a "so what" |
| **Over-claiming** | "Users want X" from 2 interviews | State confidence and sample size |
| **Skipping synthesis** | Raw notes aren't insights | Always synthesize into themes |

---

## When to Use This Skill vs. Others

| If you need to... | Use this skill | Not... |
|-------------------|----------------|--------|
| Understand user problems | Research & Synthesis | UX Design (design comes after) |
| Test if a concept resonates | Research & Synthesis | Sales (not selling, validating) |
| Decide what to prioritize | Research & Synthesis + Strategic Advisory | Strategic Advisory alone (needs evidence) |
| Understand competitive landscape | Research & Synthesis | Marketing (different lens) |
| Write up research findings | Research & Synthesis | Technical Documentation (different format) |

---

## Output Expectations

**Research report:** Answers original question, scannable structure, evidence-backed, findings separated from recommendations, actionable next steps

**Validation report:** Clear verdict (did it work?), what resonated/didn't, verbatim reactions, refined direction

**Interview guide:** Clear objectives, logical flow, mix of question types, probing questions included

**Executive summary:** 1 page max, key findings and recommendations, enough context to act

---

## When to Escalate

Flag for review when:

- Findings contradict client's strategy or assumptions
- Research reveals conflicting or ambiguous signals
- Scope expanding beyond original question
- Participants are hard to recruit (may need criteria adjustment)
- Confidence is low but stakes are high
- Ethical concerns arise
- Findings suggest pivot that affects engagement scope
